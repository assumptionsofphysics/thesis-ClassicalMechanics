\documentclass{article}
\usepackage{assumptionsofphysics}
\begin{document}

\title{Derivation Steps}

\section{Introduction}

	The purpose of this work is to re-derive Hamiltonian and Lagrangian mechanics. Historically the basis for the laws of physics were generalized experiences. These laws were rooted in the physical world rather than a mathematical one. As they are treated now, Hamiltonian and Lagrangian mechanics lack this physicality. They are taught as mathematical reformulations of Newtonian Mechanics which they are not. This work will begin with basic physical assumptions about a system in order to show it obeys Hamiltonian and Lagrangian Mechanics with a more physical justification. We will start with physical assumptions, then translate those assumptions into precise mathematical definitions. This will lead us to our results. \emph{picture with blurry and crisp analogy.}

\section{Infinitesimal Reducibility}
	
\begin{assump}[Infinitesimal reducibility]
	The state of the system is reducible to the state of its infinitesimal parts. That is, giving the state of the whole system is equivalent to giving the state of its parts, which in turn is equivalent to giving the state of its subparts and so on.
\end{assump}

	We begin with the physical condition that differentiates classical and quantum mechanics: Reducibility. In classical mechanics we consider systems which are infinitesimally reducible. This means that we can identify the state the system is in by giving the state all of its components, then the states of the components of the components and so on. These components will be our classical particles. This idea of reducibility can be tricky at first so here is clearer depiction.
	
	\emph{Explain in practical term what reducibility is. Give examples and counterexamples. Ball and red marker. Electron and photon scattering. Example needs work.}
	
	 Consider a ball of reducible material. On this ball we could draw a red dot. If we then describe the state of the red dot and the rest of the ball we have given the state of the whole ball. But if we consider a ball of irreducible material i.e. an electron, we cannot describe the state of a dot drawn on its surface. We could not say for instance that a photon scattering with that electron interacts specifically with the part of the electron with the dot. That is because the electron is not reducible; we cannot describe the state of the electron by breaking it into smaller parts and describing those parts. We cannot describe a process in which something interacts with only part of the electron; everything always interacts with its entirety. 
	 
	 \emph{Describe what the process of subdivision is, and define the particle as the limit of that process.}
	
	 Now to be clear we will still discuss the classical "particles" that make up a system but we should clarify our precise definition. These are not point particles. They are simply the limit of recursive subdivision of our reducible material. We could if we wanted continue to divide these parts further if we so chose. As we proceed understand that our definition of particle refers simply to very small regions of our homogeneous, reducible material.
	
\subsection{Constraint on Coordinate Transformations}

	Now we want to talk about the state of our composite system and the state of its particles in a precise way. Formally that can be defined as follows:	

\begin{defn}
	The state space of the whole system is $\mathcal{C}$. A particle is an infinitesimal subdivision. The state space of the particles is $\mathcal{S}$. Giving the states $\{s_1,s_2,...,s_n\} \in \mathcal{S}$ of all particles of a system is equivalent to giving the state $c \in \mathcal{C}$ of the system.
\end{defn}

	If we want to further describe the state of the system we need to identify our states in $\mathcal{S}$ with numbers so that all states are uniquely labeled. This will allow us to describe more precisely the space using things like areas and densities and how the system moves within its state space.

\begin{defn}
	A state variable assigns one numerical quantity to each state. Formally, it is a map $\xi : \mathcal{S} \to \mathbb{R}$. A complete set of state variables fully identifies a state. Formally, it is an invertible  map $\xi^\alpha : S \rightarrow \mathbb{R}^n $.
\end{defn}

	The state of the system is given by describing the state of its particles. Each state of the whole system is uniquely determined by the state of the particles meaning that given a distribution of the particles over their states, we can determine exactly the state of the whole system.
	
	Consider a box with balls of different colors. Suppose the state of the whole system will be defined by the number of balls for each color. $\mathcal{S}$ is the set of possible colors. $\rho(s)$ is the number of balls for each color. For each set of colors, we can count the balls that match that set. That is, for each $U \subset \mathcal{S}$ we have $\mu(U) = \sum_{s \in U} \rho(s)$. The whole state $c$ is determined by the function $\rho : \mathcal{S} \to \mathbb{R}$.

	If our state space is continuous, we must make two changes. First, $\rho : \mathcal{S} \to \mathbb{R}$ becomes a density and second, instead of a sum we use an integral: $\mu(U) = \int_{U} \rho(s) d\mathcal{S}$ where $d\mathcal{S}$ gives the number of states in an infinitesimal area. \emph{is this correct for $d\mathcal{S}$?}

%NOTE TO US: we are being subtle, and pretending that is just any integral, even though the fact that $\rho$ depends on the point constrains the problem.

\begin{defn}
	For each state of the composite system $c \in \mathcal{C}$ there is a unique distribution of the parts of the system over their state space. This distribution is written as $\rho_c: \mathcal{S} \rightarrow \mathbb{R}$ and describes the density per state over an infinitesimal area around $s \in \mathcal{S}$ (in the continuous case). $\mu(U)$ gives the number of states in a region $U \subset \mathcal{S}$. \emph{is this the correct way to describe $\rho$?}
\end{defn}

\emph{Discussion on units.} 
	
	Because we have now defined a distribution over our state space, it is natural to begin to discuss densities over our space. Note that densities, in general, are coordinate dependent. Consider a mass density over three dimensional space $\mathcal{M}$. We can talk about points in a coordinate independent way. Take point $A \in \mathcal{M}$; this is the same point in all references, and we do not need to even have a coordinate system defined to talk about point $A$. Now, if I have a mass distribution, and I ask you how much mass is there in a region $U \subseteq \mathcal{M}$, this is also coordinate independent . If I ask for the mass density at a point, however, I need to specify a quantity like $kg/m^3$. Formally, I have to specify a \textit{unit system} of the space. In math terms, $\rho_m : \mathbb{R}^3 \to \mathbb{R}$ not $\rho_m : \mathcal{M} \to \mathbb{R}$. This means that our density is coordinate dependent because how we construct it changes our resulting density.\footnote{Here we use "coordinate" and "unit" interchangeably. We will see later what the exact definition of a unit is, but for now they can be thought of as equivalent.}
	
	In our state space however, our density is coordinate independent. This is because the number of particles in a state does not depend on the coordinates with which states are labeled. The density is a function of state only. This means that $\rho$ changes as a scalar i.e. it doesn't change under a change of state variables because the number of particles in a particular region of state space is independent of the coordinates or units used to identify the state. For example the temperature at a point in space does not change if the units used to label that space change. Temperature is a scalar field over the space. Formally this means we can write $T : \mathcal{M} \to \mathbb{R}$. Because our density is coordinate independent we can write the density as $\rho_c : \mathcal{S} \to \mathbb{R}$ without having to specify coordinates or units.\footnote{Note that we will use $a, b, c$ for state variables; $i, j, k$ for unit variables; $\alpha, \beta, \gamma$ for unit variables that include time (i.e. four momentum). For distributions/densities we use $\rho$. States are indicated with $s$ and state variables with $\xi$. Also note the duplicity of $\rho$, $s$, and $\xi$. We use $\rho(s)$ as $\mathcal{S} \to \mathbb{R}$; $\rho(\xi^a)$ as $\mathbb{R}^n \to \mathbb{R}$; $\rho(\hat{\xi}^b)$ as $\mathbb{R}^n \to \mathbb{R}$; $\xi^a(s)$ as $\mathcal{S} \to \mathbb{R}^n$; $s(\xi^a)$ as $\mathbb{R}^n \to \mathcal{S}$.
Thus we have the following identities: $\rho(s(\xi^a)) = \rho(\xi^a)$; $\rho(s) = \rho(s (\xi^a(s)))$.}

\begin{prop}
	$\rho_c$ will transform as a scalar and a density under a change of state variables $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$.
\end{prop}

	Armed with this knowledge we can derive our mathematical constraints on changes of state variables. We will look again at the integral $\mu(U) = \int_{U} \rho(s) d\mathcal{S}$. Writing this in terms of our selected state variables we have $\mu(U) = \int_{\xi(U)} \rho(\xi^\alpha)d\xi^\alpha$. Now we consider a change of variables $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$. We then have $\mu(U) = \int_{\xi(U)} \rho(\xi^\alpha)d\xi^\alpha = \int_{\hat{\xi}(U)} \rho(\hat{\xi}^\beta)d\hat{\xi}^\beta$; we know this integral is conserved because the amount of the system occupying a region in state space does not change with a change of state variables. Using our transformation rule we can write $\int_{\hat{\xi}(U)} \rho(\hat{\xi}^\beta)d\hat{\xi}^\beta = \int_{\xi(U)} \rho(\hat{\xi}^\beta)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|d\xi^\alpha$ where $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|$ is the Jacobian of this change of state variables. So in all our equality is $\mu(U) = \int_{\xi(U)} \rho(\xi^\alpha)d\xi^\alpha = \int_{\xi(U)} \rho(\hat{\xi}^\beta)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|d\xi^\alpha$. We can drop the integral to get $\rho(\xi^\alpha) = \rho(\hat{\xi}^\beta)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|$. But because $\rho$ is a function of state only we know $\rho(\xi^\alpha) = \rho(\hat{\xi}^\beta)$ so we know $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right| = 1$. \emph{how do we show differentiability?/whats an example for contradiction?}

\begin{prop}
	A change of state variables must be differentiable and must have a unitary Jacobian i.e. $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right| = 1$.
\end{prop}

\subsection{Unit Variables, Conjugate Pairs, and 2n Dimensional State Space}

\emph{Talk about unit system in general: some choices are free, some are dependent. Give example of dependent/independent unit variables (like velocity down). Example needs refining.}

	We will now defined a unit system using a select number of our state variables which we will call unit variables. If we change one of these unit variables, we will also have to change any other state variables that depend on the definition of the unit we changed. For example if $q^1$ defines the position relative to the x-axis in meters and $k_1$ is the velocity relative to the x-axis in meters per second, a change in $q^1$ to kilometers would necessitate a change in $k_1$ to kilometers per second. So a change of units $\hat{q}^j = \hat{q}^j(q^i)$ will induce a unique change of state variables $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$. Some choices of unit variables are independent of each other i.e. charge and mass, but many are not i.e. distance and velocity along the same axis. \emph{improve this example}

\begin{defn}
	We define a unit variable $q \in \xi^a$ as a state variable that is the definition of a unit. The set of unit variables $q^i \in \xi^a$ defines a unit system upon which the other state variables depend.
\end{defn}

	Now we want to understand what happens under a change of unit variables. What is the correct corresponding change of state variables that corresponds with a change in unit variables? 
	
	Let's start with the simplest case: one variables suffices to describe our unit system. We then can write our set of state variables as $\xi^\alpha = \{q,k^\alpha\}$ where $k^\alpha$ could represent any number of state variables. Let's suppose we performed a change of units. We then have $\hat{q}^j = \hat{q}^j(q^i)$. What are the restrictions on the set of $k^\alpha$'s? We can use the fact that the Jacobian of any change of state variables must be unitary to show that there exists exactly one $k^\alpha$. 
	
	Now we allow and arbitrary number of $q^i$'s to define our unit system. Using the same process as above we show that for each $q^i$ there must be exactly one $k^\alpha$. This means that our state space $\mathcal{S}$ must be $2n$ dimensional and that our state variables come in pairs consisting of one unit variable and one dependent state variable. (The typical course in mechanics defines Hamiltonian Mechanics in terms of position-momentum pairs).\footnote{For a full, mathematically rigorous derivation see: PLACEHOLDER}

\begin{prop}
	The state space of the particles is $2n$ dimensional. The state variables are organized in pairs $\{q^i, k_i\}$ where the set $\{q^i\}$ defines the unit system. These state variables have a well defined transformation rule with a unitary Jacobian.
\end{prop}

\subsection{Degrees of Freedom, Areas in State Space, and Poisson Brackets}

\begin{defn}
	A degree of freedom is a pair $\{q^i,k_i\}$. Degrees of freedom are orthogonal 2D surfaces in phase space.
\end{defn}
	
\begin{prop}
	For each independent degree of freedom, the area given $\int_U \hbar dq^i dk_i = \int_U dq^i dp_i$ quantifies the number of possible configurations within the region $U$.
\end{prop}

\begin{defn}
	The Poisson bracket is defined as $\{f,g\} = \frac{\partial f}{\partial x}\frac{\partial g}{\partial p} - \frac{\partial g}{\partial x}\frac{\partial f}{\partial p}$.
\end{defn}

\begin{prop}
	The Poisson bracket $\{f, g\}$ translates the densities of states into densities per unit area of $f, g$. It is the Jacobian of the transformation $dfdg \rightarrow dxdp$ i.e. $dfdg = \{f,g\}dxdp$.
\end{prop}
	
\section{Reversible and Deterministic Evolution}

\begin{assump}[Reversible and Deterministic Evolution]
	The system undergoes Reversible and Deterministic Evolution meaning given the state of the system at any time, its state at all past and future times is known.
\end{assump}

Particle states change in time. But we have a law of how to go from one state to the other. Bijection $\mathcal{T}_{\Delta t} : \mathcal{S} \to \mathcal{S}$ ... This only gives you the infinitesimal displacement in time.

\begin{defn}
	We write $\lambda: \mathbb{R} \rightarrow \mathcal{S}$ defined as $s = \lambda(t)$ as the evolution of the state of one particle. Now the density associated with the state at $t_0$ is given by $\rho_c(\lambda(t_0),t_0)$.
\end{defn}

	%By our condition of Deterministic and Reversible evolution we must have $\rho(\lambda(t_0),t_0) = \rho(\lambda(t),t)$ meaning this density of states is conserved over time. So all particles that begin in the same state will evolve through and end in the same states. Now going back to our integral $\int_{\Sigma} \rho_c\omega(d\Sigma)$ we will find that our region $\Sigma$ is mapped to a new region in state space as the system evolves, but the same fraction of the parts of the system will be found in that region.
	
As particle states move, the will "drag" along the same evolution all the particles that are associated with that state. This is going to tell you that the infinitesimal displacement is "divergenceless" (conserves density)

\begin{prop}
	The density $\rho_c(s,t)$ in conserved. This means that while particles may change state over the course of the system's evolution, any particles that begin in the same state will be found in the same state throughout the evolution no matter what point in time is chosen. So we have $\rho(\lambda(t_0),t_0) = \rho(\lambda(t),t)$. (diagram showing conserved volumes/areas)
\end{prop}

\begin{prop}
	The above conditions are sufficient to say that our system is Hamiltonian and thus satisfies Hamilton's equations. So the evolution of the system can be written in the familiar way:
	$$d_tq^i = \partial_{p_i}H$$
	$$d_tp_i = -\partial_{q^i}H$$
\end{prop} 
	
	%The incredible thing here is that from two assumptions we can derive the entirety of Hamiltonian mechanics. 

\section{Kinematic Equivalence}

\begin{assump}[Kinematic Equivalence]
	(More physicsy) Trajectories in space-time suffice to recover trajectories in phase space. We know the precise trajectory in phase space given a trajectory in physical space-time; there is an invertible bijection between the two.(diagram showing bijection)
\end{assump}

\subsection{Weak Equivalence}

\begin{defn}
	We take $x^i = q^i$. We then define $v_i = d_tx^i$. Weak equivalence means $v_i(q^i, p_i)$ is invertible.
\end{defn}

(photon counter-example $v^i$ function only of $x^i$)

\begin{prop}
	A classical system that satisfies weak equivalence is a Lagrangian system. Formally, we can a define a Lagrangian $\mathcal{L}$ that is convex and has unique solutions.
\end{prop}

\subsection{Full Equivalence}

The units that are used to express the density on position and velocity must be dependent only on $q^i$ if they are to fully define the unit system.

\begin{defn}
	Full equivalence means that Kinematic Equivalence extends to the composite system. So we have $\rho(q^i,p_j) = \left|J\right|\rho(x^i,v^j) = \left|\frac{\partial v^i}{\partial p_j}\right|\rho(x^i,v^j)$. The Jacobian is only a function of $q^i$.
\end{defn}

\begin{prop}
	A system that satisfies full kinematic equivalence obeys the laws of massive particles under potential forces.
\end{prop}

\section{Advanced topics}

\subsection{Action Principle}

The action principle that many courses in mechanics take \emph{a priori} is simple a consequence of our assumptions. We can find a physical justification for our mathematical structures.

\subsection{Kinematic vs dynamics}

Differences between Newtonian/Lagrangian/Hamiltonian mechanics.

Show that they are inequivalent. Newtonian mechanics needs n functions to define the evolution while Lagrangian/Hamiltonian requires 1. All Lagrangians that have unique solution have a Hamiltonian. All Hamiltonians have unique solutions, but not necessarily a Lagrangian. (must follow discussion on the action principle) Mix with examples.

Kinematics and dynamics. Newtonian and Hamiltonian mechanics necessarily define both. Technically, Lagrangian mechanics is purely kinematic (equations are only in terms of position and velocity - you can't distinguish apparent forces from real forces). The only way to recover the dynamics is to assume the system is also Hamiltonian.

Kinematic aliasing (systems with different dynamics may have the same kinematics). Example of 3 different dynamics (friction, boosted, losing mass). This is confusing because, in some cases, you can treat non-conservative systems in Lagrangian mechanics as they look like conservative systems in a non-inertial frame (naturally, the Hamiltonian and the conjugate momentum will not correspond to what you expect).

How is kinematic aliasing resolved. Newtonian mechanics resolves kinematic aliasing by invoking inertial frames and by writing equations that are valid \emph{only} in inertial frames (you can distinguish apparent forces from real forces in an inertial frame).  In Lagrangian/Hamiltonian mechanics, the equations are invariant under coordinate transformation: we can't do that anymore. You fix the system to be conservative. If you had a system of equations that were invariant under all coordinate transformation for all frames, you would not be able to tell which forces are apparent and which forces are real.

\subsection{Entropy in classical mechanics}

Entropy is invariant under coordinate transformation - the same for all observers (invariance under time is less important).

Classical uncertainty principle: fixing the entropy of the distribution gives us an inequality that is saturated by Gaussian distributions. (discuss parallel with quantum mechanics - classical mechanics allow for minus infinite entropy)

Time evolution of entropy. It is conserved only under Hamiltonian evolution. If the evolution is not Hamiltonian, the entropy does the opposite of what one expected. (dissipative systems, like damped harmonic oscillator, concentrate the distributions around the attractor, and therefore) The problem is that the same statement at different time provides different amount of information/granularity of the description.


\section{Conclusion}
	
















\end{document}
