\documentclass{article}
\usepackage{assumptionsofphysics}
\begin{document}

\title{Derivation Steps}

\section{Introduction}

	The purpose of this work is to re-derive Hamiltonian and Lagrangian mechanics. Historically the basis for the laws of physics were generalized experiences. These laws were rooted in the physical world rather than a mathematical one. As they are treated now, Hamiltonian and Lagrangian mechanics lack this physicality. They are taught as mathematical reformulations of Newtonian Mechanics which they are not. This work will begin with basic physical assumptions about a system in order to show it obeys Hamiltonian and Lagrangian Mechanics with a more physical justification. We will start with physical assumptions, then translate those assumptions into precise mathematical definitions. This will lead us to our results. 
	
Diagram 1 here

\section{Infinitesimal Reducibility}
	
\begin{assump}[Infinitesimal reducibility]
	The state of the system is reducible to the state of its infinitesimal parts. That is, giving the state of the whole system is equivalent to giving the state of its parts, which in turn is equivalent to giving the state of its subparts and so on.
\end{assump}

	We begin with the physical condition that differentiates classical and quantum mechanics: Reducibility. In classical mechanics we consider systems which are infinitesimally reducible. This means that we can identify the state the system is in by giving the state all of its components, then the states of the components of the components and so on and \textit{vice versa}. This idea of reducibility can be tricky at first so here is a clearer depiction.
	
	\emph{Explain in practical term what reducibility is. Give examples and counterexamples. Ball and red marker. Example needs work.}
	
	Let's consider a ball that we throw through the air. This ball will follow some path through the air until it hits the ground. Now, say we draw a red dot on this ball. Then if we descirbe the motion of the ball through the air (including its angular motion), we have also described the motion of the red dot through the air. If we take the limit as the size of the red dot goes to zero and the previous statement is still true then the ball is an infinitesimally reducible system meaning giving the state of the whole is equivalent to giving the state of the infinitesimal parts. The internal dynamics of the system are accessible through knowledge of its external dynamics.
	
	 %Consider a ball of reducible material. On this ball we could draw a red dot. If we then describe the state of the red dot and the rest of the ball we have given the state of the whole ball. But if we consider a ball of irreducible material i.e. an electron, we cannot describe the state of a dot drawn on its surface. We could not say for instance that a photon scattering with that electron interacts specifically with the part of the electron with the dot. That is because the electron is not reducible; we cannot describe the state of the electron by breaking it into smaller parts and describing those parts. We cannot describe a process in which something interacts with only part of the electron; everything always interacts with its entirety. 
	 
	 \emph{Describe what the process of subdivision is, and define the particle as the limit of that process. improve clarity + example}
	
	 Now to be clear when we discuss the classical "particles" that make up a system we have a precise definition. We take our system and continually divide it until parts of the system are small enough that we can say they occupy one state in their state space. 
	 \emph{potential example: For example let's consider a system of blocks attached to each other in a line with springs. This is a reducible system because I can describe the state of the system as a whole by describing the states of the blocks and describing the states of the blocks is the same as giving the state of the whole. Now let's divide this system into individual blocks. These will be our classical particles. We can say that an individual block is in precisely one state. We could in principle continue and divide into smaller and smaller blocks, but the resulting subdivisions wouldn't do anything. The blocks are the limit of our recursive subdivision.keep???}
	 
	 Going back to our ball and red dot example, we take the limit as the size of the dot goes to zero, but it never actually reaches zero i.e. the dot has some extent and is a classical particle. Evidently this description of a particle is not at a point but rather an infinitesimal region in state space (we will see what a state space is later).
	
	Now we know that the state of the parts is given by describing the state of the whole. This relationship holds the other way as well. If we describe the state of the parts we have described the state of the whole. Each state of the whole system is uniquely determined by the states of the particles meaning that given a distribution of the particles over their states, we can determine exactly the state of the whole system. Let's formalize this idea.
	
	Let's start in the case of a discrete system as it is conceptually simpler. Consider a box with balls of different colors as our composite system. Suppose the state of the whole system will be defined by the number of balls for each color. We can formally define the \textit{state space} of the balls (our classical particles in this case) as $\mathcal{S}$ which spans all of the possible colors of the balls. We can define the number of balls of each color as a \textit{distribution}, $\rho$, over this state space $\mathcal{S}$. For each set of colors, we can count the balls that are in that set; call this function $\mu : \mathcal{S} \to \mathbb{R}$. That is, for each $U \subset \mathcal{S}$ we have $\mu(U) = \sum_{s \in U} \rho(s)$. The state space of the composite system is $\mathcal{C}$. For each state $c \in \mathcal{C}$ we have exactly one $\rho$.
	
Diagrams 2, 3 here
	
	\emph{more physical justification needed?}

	If our state space is continuous the main ideas of our justification above hold, but we must make two changes to the formalization. First, $\rho : \mathcal{S} \to \mathbb{R}$ becomes a density of states and second, for $\mu$ instead of a sum we use an integral: $\mu(U) = \int_{U} \rho(s) d\mathcal{S}$ where $d\mathcal{S}$ gives the number of states in an infinitesimal area. \emph{is this correct for $d\mathcal{S}$?} We will continue to work in the continuous case unless otherwise stated.
	
\begin{defn}
	Let $\mathcal{C}$ be the state space of a system. The system is \textbf{infinitesimally reducible} to the infinitesimal parts (i.e. particles) identified by the state space $\mathcal{S}$ if there exists a measure $dS$ such that for every $c \in \mathcal{C}$ we can find a $\rho : \mathcal{S} \to \mathbb{R}$ such that for every $U \subseteq \mathcal{S}$ the integral $\mu(U) = \int_U \rho(s) dS$ corresponds to the fraction of the system whose parts occupy those states, which means $\mu(S) = 1$.\footnote{This may seem arbitrarily complex at first, but we will see exactly what all of these mathematical objects are shortly and why this level of precision is necessary.}
\end{defn}	
	
\subsection{Constraint on Coordinate Transformations}	

	If we want to further describe the state of the system we need to identify our states in $\mathcal{S}$ with numbers so that all states are uniquely labeled. This will allow us to describe more precisely how the system moves within this state space. For example, consider a cannon that fires a collection of cannon balls. Let's say that these balls all land at different places along an axis. Without defining a \textit{state variable} we can only say that the balls landed in some order; we have no notion of how far apart any of them are. To be specific the distance along our axis is our state variable. It defines a set of numerical values that label our state space. Without this labeling we lack a precise way to compare the states of the individual particles.

\begin{defn}
	A state variable assigns one numerical quantity to each state. Formally, it is a map $\xi : \mathcal{S} \to \mathbb{R}$. A complete set of state variables fully identifies a state. Formally, it is an invertible  map $\xi^a : S \rightarrow \mathbb{R}^n $.
\end{defn}

\emph{Discussion on units. Change order around put coordinate dependence end of 1st paragraph. Clarify density vs distribution; invariant vs dependent.} 
	
	Because we have now defined a distribution and a set of state variables over our state space it is natural to write this distribution $\rho$ in terms of those state variables $\xi^a$. Because we know $\xi^a$ is an invertible map between $\mathbb{R}^n$ and our state space $\mathcal{S}$ we can write its inverse as $\xi^{-1} : \mathbb{R}^n \to \mathcal{S}$. Taking this we can compose $\rho(s)$ with $s = \xi^{-1}(\xi^a)$ to get $\rho : \mathbb{R}^n \to \mathbb{R}$.\footnote{Note that we will use $\xi^a, \xi^b, \xi^c$ for state variables; $q^i, q^j, q^k$ for unit variables; $\xi^\alpha, \xi^\beta, \xi^\gamma$ for state variables that include time (i.e. four momentum). For distributions/densities we use $\rho$. States are indicated with $s$ and state variables with $\xi$. Also note the duplicity of $\rho$, $s$, and $\xi$. We use $\rho(s)$ as $\mathcal{S} \to \mathbb{R}$; $\rho(\xi^a)$ as $\mathbb{R}^n \to \mathbb{R}$; $\rho(\hat{\xi}^b)$ as $\mathbb{R}^n \to \mathbb{R}$; $\xi^a(s)$ as $\mathcal{S} \to \mathbb{R}^n$; $s(\xi^a)$ as $\mathbb{R}^n \to \mathcal{S}$.
Thus we have the following identities: $\rho(s(\xi^a)) = \rho(\xi^a)$; $\rho(s) = \rho(s (\xi^a(s)))$.} This will give us a density of states over our state space (in the continuous case). We can then write our $\mu(U)$ as $\mu(U) = \int_{\xi(U)} \rho(\xi^a)d\xi^a$. If we change our state variables we know that that fraction of the system in a region $U \subset \mathcal{S}$ will not change. Let $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$ be our change of variables. This gives us $\mu(U) = \int_{\xi(U)} \rho(\xi^a)d\xi^a = \int_{\hat{\xi}(U)} \rho(\hat{\xi}^b)d\hat{\xi}^b$. Using our transformation rule we can write $\int_{\hat{\xi}(U)} \rho(\hat{\xi}^b)d\hat{\xi}^b = \int_{\xi(U)} \rho(\hat{\xi}^b)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|d\xi^a$ where $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|$ is the Jacobian of this change of state variables. We know that this change of variables must be differentiable otherwise the Jacobian will not be well defined. We also know that $\rho$ transforms as a density under change of variables. \emph{how to justify transform as density?}

	Now let's consider a more typical example of a density: mass distributed over three dimensional space $\mathcal{M}$. We can talk about points in a coordinate independent way.\footnote{Throughout this work we will use "coordinate" and "state variable" interchangeably.} Take a point $A \in \mathcal{M}$; this is the same point in all references, and we do not need to even have a coordinate system defined to talk about point $A$. Now, if I have a mass distribution, and I ask you how much mass is there in a region $U \subseteq \mathcal{M}$, this is also coordinate independent as the definition of $U$ does not require a coordinate system. If I ask for the mass density at a point, however, I need to specify a quantity like $kg/m^3$. Formally, I have to specify a \textit{unit system} of the space. In math terms this means that $\rho_m : \mathbb{R}^3 \to \mathbb{R}$ not $\rho_m : \mathcal{M} \to \mathbb{R}$. We cannot define a map directly from $\mathcal{M}$. If we were to change our unit system to $g/km^3$ the actual numerical value of the density at each point changes; the density varies with the choice of coordinates. So in this example we have seen a quantity that is neither coordinate invariant nor independent because it needs a coordinate system to be well defined and its numerical value depends on the choice of that coordinate system.\footnote{Here we used "coordinate" and "unit" interchangeably. We will see later what the exact definition of a unit is, but for now they can be thought of as equivalent.}
	
Diagram 4 here
	
	What does a coordinate independent quantity look like? One example is temperature. We can give the temperature at a point in space without a coordinate system; it is coordinate independent. There is no need for a conception of distance or any other unit to define the temperature. Formally this means we can write $T : \mathcal{M} \to \mathbb{R}$; this is a direct map from space-time to a number, we do not need to label space-time using a coordinate system. This independence also means logically that changing our coordinate system does not change the numerical value of temperature at a point. So temperature is invariant under change of coordinates.\footnote{We say coordinate independent to mean can be defined without a specified coordinate system; to be coordinate invariant means there will be no change under a change of variables. We can have coordinate dependent quantities that are coordinate invariant, but coordinate independent quantities are always coordinate invariant.} What properties does our density $\rho$ exhibit?
	
	The number of particles in a state can be described without using a coordinate system. We can say that a certain amount of the system is in a specific state without labeling that state with numbers. This means that our density is coordinate independent. Our density is a function of state only. Because of this the numerical value of the density at a point is not changed when we change coordinate systems because we do not change how many particles are in each state, only how we label those states. So $\rho$ is coordinate invariant and will transform as a scalar under a change of variables i.e. $\rho(\xi^a) = \rho(\hat{\xi}^b)$. \emph{reference diagram 3, maybe draw another for this as well? use diagram 5?}

\begin{prop}
	Because it is coordinate independent and invariant, $\rho$ will transform as a scalar and a density under a change of state variables $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$.
\end{prop}

	Armed with this knowledge we can derive our mathematical constraints on changes of state variables. We already know $\rho(\xi^a) = \rho(\hat{\xi}^b)$ by proposition 3; we can drop the integral from $\int_{\hat{\xi}(U)} \rho(\hat{\xi}^b)d\hat{\xi}^b = \int_{\xi(U)} \rho(\hat{\xi}^b)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|d\xi^a$ to get $\rho(\xi^a) = \rho(\hat{\xi}^a)\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right|$ which implies that $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right| = 1$ and is dimensionless.\emph{where should we make the point that the units of $\rho$ don't change?}
	
Diagram 6 here
	
\begin{prop}
	A change of state variables must be differentiable and must have a unitary Jacobian i.e. $\left|\frac{\partial\hat{\xi}^b}{\partial\xi^a}\right| = 1$.
\end{prop}

\subsection{Unit Variables, Conjugate Pairs, and 2n Dimensional State Space}

\emph{Talk about unit system in general: some choices are free, some are dependent. Give example of dependent/independent unit variables (like velocity down). Example needs refining.}

	We will now defined a unit system using a select number of our state variables which we will call unit variables. If we change one of these unit variables, we will also have to change any other state variables that depend on the definition of the unit we changed. For example if $q^1$ defines the position relative to the x-axis in meters and $k_1$ is the velocity relative to the x-axis in meters per second, a change in $q^1$ to kilometers would necessitate a change in $k_1$ to kilometers per second. So a change of units $\hat{q}^j = \hat{q}^j(q^i)$ will induce a unique change of state variables $\hat{\xi}^b = \hat{\xi}^b(\xi^a)$. Some choices of unit variables are independent of each other i.e. charge and mass, but many are not i.e. distance and velocity along the same axis. \emph{improve this example}

\begin{defn}
	We define a unit variable $q \in \xi^a$ as a state variable that is the definition of a unit. The set of unit variables $q^i \in \xi^a$ defines a unit system upon which the other state variables depend.
\end{defn}

	Now we want to understand what happens under a change of unit variables. What is the correct corresponding change of state variables that corresponds with a change in unit variables? 
	
	Let's start with the simplest case: one variables suffices to describe our unit system. We then can write our set of state variables as $\xi^a = \{q,k^a\}$ where $k^a$ could represent any number of state variables. Let's suppose we performed a change of units. We then have $\hat{q}^j = \hat{q}^j(q^i)$. What are the restrictions on the set of $k^a$'s? We can use the fact that the Jacobian of any change of state variables must be unitary to show that there exists exactly one $k^a$. 
	
	Now we allow and arbitrary number of $q^i$'s to define our unit system. Using the same process as above we show that for each $q^i$ there must be exactly one $k^a$. This means that our state space $\mathcal{S}$ must be $2n$ dimensional and that our state variables come in pairs consisting of one unit variable and one dependent state variable. (The typical course in mechanics defines Hamiltonian Mechanics in terms of position-momentum pairs).\footnote{For a full, mathematically rigorous derivation see: PLACEHOLDER}

\begin{prop}
	The state space of the particles is $2n$ dimensional. The state variables are organized in pairs $\{q^i, k_i\}$ where the set $\{q^i\}$ defines the unit system. These state variables have a well defined transformation rule with a unitary Jacobian.
\end{prop}

\subsection{Degrees of Freedom, Areas in State Space, and Poisson Brackets}

\begin{defn}
	A degree of freedom is a pair $\{q^i,k_i\}$. Degrees of freedom are orthogonal 2D surfaces in phase space.
\end{defn}
	
\begin{prop}
	For each independent degree of freedom, the area given $\int_U \hbar dq^i dk_i = \int_U dq^i dp_i$ quantifies the number of possible configurations within the region $U$.
\end{prop}

\begin{defn}
	The Poisson bracket is defined as $\{f,g\} = \frac{\partial f}{\partial x}\frac{\partial g}{\partial p} - \frac{\partial g}{\partial x}\frac{\partial f}{\partial p}$.
\end{defn}

\begin{prop}
	The Poisson bracket $\{f, g\}$ translates the densities of states into densities per unit area of $f, g$. It is the Jacobian of the transformation $dfdg \rightarrow dxdp$ i.e. $dfdg = \{f,g\}dxdp$.
\end{prop}
	
\section{Reversible and Deterministic Evolution}

\begin{assump}[Reversible and Deterministic Evolution]
	The system undergoes Reversible and Deterministic Evolution meaning given the state of the system at any time, its state at all past and future times is known.
\end{assump}

Particle states change in time. But we have a law of how to go from one state to the other. Bijection $\mathcal{T}_{\Delta t} : \mathcal{S} \to \mathcal{S}$ ... This only gives you the infinitesimal displacement in time.

\begin{defn}
	We write $\lambda: \mathbb{R} \rightarrow \mathcal{S}$ defined as $s = \lambda(t)$ as the evolution of the state of one particle. Now the density associated with the state at $t_0$ is given by $\rho_c(\lambda(t_0),t_0)$.
\end{defn}

	%By our condition of Deterministic and Reversible evolution we must have $\rho(\lambda(t_0),t_0) = \rho(\lambda(t),t)$ meaning this density of states is conserved over time. So all particles that begin in the same state will evolve through and end in the same states. Now going back to our integral $\int_{\Sigma} \rho_c\omega(d\Sigma)$ we will find that our region $\Sigma$ is mapped to a new region in state space as the system evolves, but the same fraction of the parts of the system will be found in that region.
	
As particle states move, the will "drag" along the same evolution all the particles that are associated with that state. This is going to tell you that the infinitesimal displacement is "divergenceless" (conserves density)

\begin{prop}
	The density $\rho_c(s,t)$ in conserved. This means that while particles may change state over the course of the system's evolution, any particles that begin in the same state will be found in the same state throughout the evolution no matter what point in time is chosen. So we have $\rho(\lambda(t_0),t_0) = \rho(\lambda(t),t)$. (diagram showing conserved volumes/areas)
\end{prop}

\begin{prop}
	The above conditions are sufficient to say that our system is Hamiltonian and thus satisfies Hamilton's equations. So the evolution of the system can be written in the familiar way:
	$$d_tq^i = \partial_{p_i}H$$
	$$d_tp_i = -\partial_{q^i}H$$
\end{prop} 
	
	%The incredible thing here is that from two assumptions we can derive the entirety of Hamiltonian mechanics. 

\section{Kinematic Equivalence}

\begin{assump}[Kinematic Equivalence]
	(More physicsy) Trajectories in space-time suffice to recover trajectories in phase space. We know the precise trajectory in phase space given a trajectory in physical space-time; there is an invertible bijection between the two.(diagram showing bijection)
\end{assump}

\subsection{Weak Equivalence}

\begin{defn}
	We take $x^i = q^i$. We then define $v_i = d_tx^i$. Weak equivalence means $v_i(q^i, p_i)$ is invertible.
\end{defn}

(photon counter-example $v^i$ function only of $x^i$)

\begin{prop}
	A classical system that satisfies weak equivalence is a Lagrangian system. Formally, we can a define a Lagrangian $\mathcal{L}$ that is convex and has unique solutions.
\end{prop}

\subsection{Full Equivalence}

The units that are used to express the density on position and velocity must be dependent only on $q^i$ if they are to fully define the unit system.

\begin{defn}
	Full equivalence means that Kinematic Equivalence extends to the composite system. So we have $\rho(q^i,p_j) = \left|J\right|\rho(x^i,v^j) = \left|\frac{\partial v^i}{\partial p_j}\right|\rho(x^i,v^j)$. The Jacobian is only a function of $q^i$.
\end{defn}

\begin{prop}
	A system that satisfies full kinematic equivalence obeys the laws of massive particles under potential forces.
\end{prop}

\section{Advanced topics}

\subsection{Action Principle}

The action principle that many courses in mechanics take \emph{a priori} is simple a consequence of our assumptions. We can find a physical justification for our mathematical structures.

\subsection{Kinematic vs dynamics}

Differences between Newtonian/Lagrangian/Hamiltonian mechanics.

Show that they are inequivalent. Newtonian mechanics needs n functions to define the evolution while Lagrangian/Hamiltonian requires 1. All Lagrangians that have unique solution have a Hamiltonian. All Hamiltonians have unique solutions, but not necessarily a Lagrangian. (must follow discussion on the action principle) Mix with examples.

Kinematics and dynamics. Newtonian and Hamiltonian mechanics necessarily define both. Technically, Lagrangian mechanics is purely kinematic (equations are only in terms of position and velocity - you can't distinguish apparent forces from real forces). The only way to recover the dynamics is to assume the system is also Hamiltonian.

Kinematic aliasing (systems with different dynamics may have the same kinematics). Example of 3 different dynamics (friction, boosted, losing mass). This is confusing because, in some cases, you can treat non-conservative systems in Lagrangian mechanics as they look like conservative systems in a non-inertial frame (naturally, the Hamiltonian and the conjugate momentum will not correspond to what you expect).

How is kinematic aliasing resolved. Newtonian mechanics resolves kinematic aliasing by invoking inertial frames and by writing equations that are valid \emph{only} in inertial frames (you can distinguish apparent forces from real forces in an inertial frame).  In Lagrangian/Hamiltonian mechanics, the equations are invariant under coordinate transformation: we can't do that anymore. You fix the system to be conservative. If you had a system of equations that were invariant under all coordinate transformation for all frames, you would not be able to tell which forces are apparent and which forces are real.

\subsection{Entropy in classical mechanics}

Entropy is invariant under coordinate transformation - the same for all observers (invariance under time is less important).

Classical uncertainty principle: fixing the entropy of the distribution gives us an inequality that is saturated by Gaussian distributions. (discuss parallel with quantum mechanics - classical mechanics allow for minus infinite entropy)

Time evolution of entropy. It is conserved only under Hamiltonian evolution. If the evolution is not Hamiltonian, the entropy does the opposite of what one expected. (dissipative systems, like damped harmonic oscillator, concentrate the distributions around the attractor, and therefore) The problem is that the same statement at different time provides different amount of information/granularity of the description.


\section{Conclusion}
	
















\end{document}
